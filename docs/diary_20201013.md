# 20201013

## 概要
- Notebook追うのはしんどいので高Voteのみにしようかな
- Datatableで読み込み→feather
- DSB2019の解法みてたらNNで解くのもありなきがしてきた(3位: https://naotaka1128.hatenadiary.jp/entry/dsb-2019-top-solution)
- 公開Notebookの手法、Leakしすぎでは？
  - [target encodingのアンチパターン](https://blog.amedama.jp/entry/target-mean-encoding-types)


## やりたいこと
- データの読み込み
- 1人1人の分析
  - 勉強を続けると回答率が上がるか
  - 講義は回答率に影響するか
- Validation戦略を考える
- 特徴量でListerningとLeadingを追加
- Statな手法をpartごとに分けて予測
- オンライン学習([Passive Aggressive](https://qiita.com/Wotipati/items/a8eda3f246eb07c516ca)や[SCW-II
](https://dev.classmethod.jp/articles/soft-confidence-weighted-impl/))
  - [まとめ](http://soonraah.hatenablog.com/entry/2016/06/06/230521)

### 特徴量
- questionから
  - part(ohe: 0~6)
  - Listening/Reading(ohe: 0/1)
  - tag features(embedding)
- lectureから
  - part(ohe: 0~6)
  - type_of(ohe: 0~3) conceptとsolving questionに絞ってもいいかも？
    - concept: 222
    - solving question: 186
    - intention: 7
    - starter: 3
  - tag(多くても7とかでユニーク多いので使わないでもよいかも)
 
## Discussion & Notebook

### データの読み込み
- https://www.kaggle.com/currypurin/riiid-feather-format-vs-pickle-format
  - feather形式にして保存が良さそう？
- 色々まとまっててGood: https://www.kaggle.com/rohanrao/tutorial-on-reading-large-datasets

### ニューラルネット
- Kerasとかpytorchとか色々実装がある
- 全結合がメインっぽい
- RNN系とかTransformerとかつかえるかもだけどよくわからん
- LGBMで戦う方がいいかなぁ
  - GBDT系まとめ: https://www.kaggle.com/code1110/riiid-gbdt-pipeline-baseline
  - optunaつかったチューニング: https://www.kaggle.com/code1110/riiid-lgb-hyperparameter-tuning
    - LightGBM Tunerでよくね？

### Tabnet
- pytorch実装のTable用NN
- https://github.com/dreamquark-ai/tabnet/
- https://www.kaggle.com/alexj21/riiid-tabnet-starter-kernel

### privateの検証
- question.csvのidは同じ
- lecture.csvのidは同じ
- すべてのquestionのidはquestion.csvに含まれる
- すべてのlectureのidはlecture.csvに含まれる
- timestampはtrainの続き
- RNNとかTransformerとか使えそう？

### Validationセットの作り方
- [DoubleValidation](https://www.kaggle.com/ilialar/riiid-5-folds-double-validation)←本当にやりたいのは多分こっち
  - よさげ
  - けっこうメモリくう
  - 20%のvalidationに新しいユーザーと、既存のユーザーの最新データも入れてあげる
- [現状BESTのKernel](https://www.kaggle.com/dwit392/expanding-on-simple-lgbm)
  - これもまぁまぁ良さげ
  - 最新1件のみなので1.5%とかしか無いから要改善
