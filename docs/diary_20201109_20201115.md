# diary_20201109_20201115
## 実験
|モデル名|Loss|AUC|LB|memo|
|--|--|--|--|--|
|xgb_v11_05|0.5494|0.7499|0.757|baseline|
|xgb_v11_06|0.5474|0.7522|0.760|baseline, DEPTH=7|
|xgb_v11_07|0.5462|0.7536|0.761|baseline, DEPTH=9|
|xgb_v13_02|0.5382|0.7534|0.742|先週BEST. たぶんlectureでleakしてる|
|xgb_v13_03|0.5383|0.7533|0.743|lectureのleakを修正|
|xgb_v13_04|0.5488|0.7507||baselineの再現|
|__xgb_v13_05__|__0.5459__|__0.7540__||__baseline, DEPTH=9→新しいbaselineにする__|
|xgb_v13_06|0.5451|0.7550||累計特徴(work, correct)の追加|
|xgb_v13_07||||累計特徴(work)の追加|

## 課題
### lectureモデルのLBスコアが低い
- feature更新モデルっぽい→[Notebook](https://www.kaggle.com/its7171/lgbm-with-loop-feature-engineering)

### 特徴量
- timediff
- ミスってるときに選びやすい番号とか？
- bundle_idで一緒に出る数とか
- shapとかで重要度計算
- bundle_idとか
  - ユーザーごとに出される問題にパターンがありそう？
  - 次に出される問題を予測してみるとか
- prior_question_*
  - 同じ問題を繰り返し出してたりする(確認のため？)
- partそのままいれてもいいかも
  - GBDTだとoheしなくても解釈してくれるそう→[Discussion](https://www.kaggle.com/its7171/lgbm-with-loop-feature-engineering#1072146)
### モデル
- NNとか試す
- Transformer
  - [SAINT(元論文)の解説Discussion](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/195632)

## メモ

### Knowlege Tracking
- まとめDiscussio: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/189962
