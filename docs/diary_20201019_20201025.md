# 20201019_20201025

## 課題
- BigQueryで特徴量生成→notebookで学習、をやりたかったけど、メモリが厳しそう
  - BigQuery MLで頑張る
- BigQuery MLとNotebookの再現性が取れない
  - とりあえずシンプルなモデルで推論が再現できる確認する
    - できて、再現性が確保できていることを確認した
    - 確認コードのバグだったかも？
- データ(lecture.csv)が更新されたそう
- カテゴリカルなデータをそういう形にしてみる？Label Encoding?
  - Stringにキャストすれば良さそう
- 欠損値、全部-1で埋めてるけどそれで大丈夫？
  - `トレーニングと予測の両方で、数値列の NULL 値は、元の入力データの特徴列によって計算された平均値に置き換えられます。`とのこと([Docs](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#imputation))
- [入力値について](http://sucrose.hatenablog.com/entry/2018/09/23/235525)
  - 自動で標準化されるそう
  - NNとか使う時に気をつけたい
- `feat_u_v2`と`feat_p_v1`のfold-1〜fold-4を抽出してない
- 特徴量を増やすと提出でエラーになる

## 特徴量
- prior_question_elapsed_timeとtimestampの差
  - `timediff_start`として追加
- lecture使ったなにか(tagがquestionと統合されたので)
- tagをつかったembedding
- Listening/Reading(ohe: 0/1)
  - `part_type`として追加
- k-meansやLDA、PCAによる特徴量追加(ナナメの特徴)
- timestampのdiff(もっと大きく)
  - 2^で2〜16
- partごとの正解率
  - 全体では作成
  - ユーザーごとのpartごとの正解率とかも入れたい
- 歪度(Skewness)と尖度(Kurtosis)などの基本統計量
  - [このページ](https://learnsql.com/blog/high-performance-statistical-queries-skewness-kurtosis/)見ると良さそう
- 正解率の移動平均とか考えたけど、他のユーザーのが入ってしまう可能性があるのでNG
- 各パート、各タイプの講義を1回以上視聴したことを示すフラグ(11の特徴)
- 少なくとも1回の講義を視聴したことを示すフラグ
- 質問データセットの 'bundle_id' と 'part' をマージする
- prior_question_elapsed_time' を平均値で埋める
- target encodiing
  - task_container_id
  - bundle_id


## Discussion & Notebook
### [Old lecture.csv for fairness](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191530)
- 古いテストIDが置かれてる
### [Simple LGBM 3 Feature Model](https://www.kaggle.com/dwit392/simple-lgbm-3-feature-model)
- 3つの特徴量だけのシンプルなモデル
- [v19](https://www.kaggle.com/dwit392/simple-lgbm-3-feature-model?scriptVersionId=44299556)でValid 0.7555477308738588 / LB 0.738
- 拡張版: [Expanding on Simple LGBM](https://www.kaggle.com/dwit392/expanding-on-simple-lgbm)
### [RIIID: FTRL FTW !](https://www.kaggle.com/rohanrao/riiid-ftrl-ftw)
- 広告で使われるオンライン学習モデル
- よさげ
- DIscussion: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191751
### [A possible solution to partial_fit on chunks rather than on a single batch!](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191856)
- オンライン学習
- チャンクを使う？

### [Does a GrowNet could be useful in this competition?](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191626)
- 購買BoostingっぽいNN

### [Submission scoring error - possible reasons collection](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/192124)
- submission errorのまとめ

### [Riiid! Answer Correctness Prediction](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/192137)
- 特徴量スレ

## メモ
### [Null importance](https://www.kaggle.com/takamichitoda/riiid-xgboost-null-importance?scriptVersionId=45085355)
- v1の特徴量は全部使えそう
### 新規ユーザーの扱い
- レコメンド系のアルゴリズムを使って類似ユーザーと同じ値を入れるとか

### 実験
Now Best Fold-0: 0.7435684315684316 / LB: 0.709

#### [3特徴量に絞って](https://www.kaggle.com/takamichitoda/riiid-xgboost-infer?scriptVersionId=45016693)
- Fold-0: 0.7035 / LB: 0.636

#### [3特徴量に絞って + Depth=6](https://www.kaggle.com/takamichitoda/riiid-xgboost-infer?scriptVersionId=45075887)
- Fold-0: 0.7161 / LB: 0.650
- 深くしたほうが良さそう
- とりあえず今は5で固定しとく？

#### [v1特徴量すべて](https://www.kaggle.com/takamichitoda/riiid-xgboost-infer?scriptVersionId=45079175)
- 欠損値は埋めた
- fold-0: 0.7427 / LB: 0.663

#### [v1特徴量すべて + カテゴリカル変数をohe](https://www.kaggle.com/takamichitoda/riiid-xgboost-infer?scriptVersionId=45082036)
- fold-0: 0.7428 / LB: 

#### [v2特徴量追加](https://www.kaggle.com/takamichitoda/riiid-xgboost-infer?scriptVersionId=45092825)
- fold-0: 0.7429 / LB: XXX

#### v2特徴量追加 + Depth=7
- fold-0: 0.7431 / LB: XXX

#### v2特徴量追加 + Depth=8
- fold-0: 0.7434 / LB:XXX

#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.4
- fold-0: 0.7433　/ LB: XXX

#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.2
- fold-0: 0.7435 / LB: XXX

#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.1
- fold-0: 0.7438 / LB: XXX


#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.1, MIN_TREE_CHILD_WEIGHT=2
- fold-0: 0.7438 / LB: XXX

#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.1, MIN_TREE_CHILD_WEIGHT=4
- fold-0: 0.7437 / LB: XXX

#### v2特徴量追加 + Depth=8, COLSAMPLE_BYLEVEL=0.1, MIN_TREE_CHILD_WEIGHT=2, LR=0.3
- fold-0:  / LB:

### XGBoostのパラメータ([10/17](https://github.com/trtd56/Riiid/blob/master/docs/diary_20201012_20201018/diary_20201017.md)の残り)
- MIN_SPLIT_LOSS = float64_value,
  - ツリーの葉にさらに分割するために必要な最小損失削減
  - 大きいほど、アルゴリズムは保守的≒過学習しにくい
  - デフォルト値は 0
- MAX_TREE_DEPTH = int64_value,
  - ツリーの最大深度
  - デフォルト値は 6
- SUBSAMPLE = float64_value,
  - 学習のサブサンプリング率
  - 例えばこの値を 0.5 に設定すると、ツリーを拡大する前に学習データの半分がランダムにサンプリングされ、過学習を防ぐ
  - サブサンプリングは、ブースト イテレーションごとに 1 回行われる
  - テストデータの分割とは関係ない（デフォルトでは 80 / 20 ランダム）
  - デフォルト値は 1.0 （各イテレーションですべてのトレーニング データが使用される）
- AUTO_CLASS_WEIGHTS = { TRUE | FALSE },
  - 各クラスの頻度に反比例した重みを使用して、クラスラベルのバランスをとるかどうか
  - デフォルト False
- CLASS_WEIGHTS = struct_array,
  - 各クラスラベルに使用する重み
  - STRUCT の ARRAY
  - 例: `CLASS_WEIGHTS = [STRUCT('example_label', .2)]`
- L1_REG = float64_value,
  - 適用する L1 正則化の量
  - デフォルト 0
- L2_REG = float64_value,
  - 適用する L2 正則化の量
  - デフォルト 1.0
- EARLY_STOP = { TRUE | FALSE },
  - early stopを使用するか否か
- LEARN_RATE = float64_value,
  - 学習率
  - デフォルト値 0.3
- INPUT_LABEL_COLS = string_array,
  - 目的変数の列名
- MAX_ITERATIONS = int64_value,
  - ブーストのラウンドの最大数
  - デフォルト20
- MIN_REL_PROGRESS = float64_value,
  - EARLY_STOP が true に設定されている場合に、トレーニングの継続に最低限必要な相対損失改善値
  - たとえば、値を 0.01 に設定した場合、イテレーションごとに損失が 1% 減少すると、トレーニングが継続する。
  - デフォルト値は 0.01

#### その他参考
- [XGBoost公式](https://xgboost.readthedocs.io/en/latest/parameter.html)
- [BigQuery ML XGBoost](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)
- [ハイパラチューニング記事１](https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f)
- [ハイパラチューニング記事2](https://qiita.com/FJyusk56/items/0649f4362587261bd57a)
- [ハイパラチューニング記事3](https://shengyg.github.io/repository/machine%20learning/2017/02/25/Complete-Guide-to-Parameter-Tuning-xgboost.html)

パタメータチューニングに関してはKaggle本呼んだほうが良いかも
