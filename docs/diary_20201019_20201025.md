# 20201019_20201025

## 課題
- BigQueryで特徴量生成→notebookで学習、をやりたかったけど、メモリが厳しそう
- BigQuery MLとNotebookの再現性が取れない
  - とりあえずシンプルなモデルで推論が再現できる確認する
- データ(lecture.csv)が更新されたそう
## 特徴量
- prior_question_elapsed_timeとtimestampの差

## Discussion & Notebook
### [Old lecture.csv for fairness](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191530)
- 古いテストIDが置かれてる
### [Expanding on Simple LGBM](https://www.kaggle.com/dwit392/expanding-on-simple-lgbm)
- 3つの特徴量だけのシンプルなモデル
### [RIIID: FTRL FTW !]()
- 広告で使われるオンライン学習モデル
- よさげ
- DIscussion: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191751

### [Does a GrowNet could be useful in this competition?](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191626)
- 購買BoostingっぽいNN

## メモ

### XGBoostのパラメータ([10/17](https://github.com/trtd56/Riiid/blob/master/docs/diary_20201012_20201018/diary_20201017.md)の残り)
- MIN_SPLIT_LOSS = float64_value,
  - ツリーの葉にさらに分割するために必要な最小損失削減
  - 大きいほど、アルゴリズムは保守的≒過学習しにくい
  - デフォルト値は 0
- MAX_TREE_DEPTH = int64_value,
  - ツリーの最大深度
  - デフォルト値は 6
- SUBSAMPLE = float64_value,
  - 学習のサブサンプリング率
  - 例えばこの値を 0.5 に設定すると、ツリーを拡大する前に学習データの半分がランダムにサンプリングされ、過学習を防ぐ
  - サブサンプリングは、ブースト イテレーションごとに 1 回行われる
  - テストデータの分割とは関係ない（デフォルトでは 80 / 20 ランダム）
  - デフォルト値は 1.0 （各イテレーションですべてのトレーニング データが使用される）
- AUTO_CLASS_WEIGHTS = { TRUE | FALSE },
  - 各クラスの頻度に反比例した重みを使用して、クラスラベルのバランスをとるかどうか
  - デフォルト False
- CLASS_WEIGHTS = struct_array,
  - 各クラスラベルに使用する重み
  - STRUCT の ARRAY
  - 例: `CLASS_WEIGHTS = [STRUCT('example_label', .2)]`
- L1_REG = float64_value,
  - 適用する L1 正則化の量
  - デフォルト 0
- L2_REG = float64_value,
  - 適用する L2 正則化の量
  - デフォルト 1.0
- EARLY_STOP = { TRUE | FALSE },
  - early stopを使用するか否か
- LEARN_RATE = float64_value,
  - 学習率
  - デフォルト値 0.3
- INPUT_LABEL_COLS = string_array,
  - 目的変数の列名
- MAX_ITERATIONS = int64_value,
  - ブーストのラウンドの最大数
  - デフォルト20
- MIN_REL_PROGRESS = float64_value,
  - EARLY_STOP が true に設定されている場合に、トレーニングの継続に最低限必要な相対損失改善値
  - たとえば、値を 0.01 に設定した場合、イテレーションごとに損失が 1% 減少すると、トレーニングが継続する。
  - デフォルト値は 0.01

#### その他参考
- [XGBoost公式](https://xgboost.readthedocs.io/en/latest/parameter.html)
- [ハイパラチューニング記事１](https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f)
- [ハイパラチューニング記事2](https://qiita.com/FJyusk56/items/0649f4362587261bd57a)
パタメータチューニングに関してはKaggle本呼んだほうが良いかも
