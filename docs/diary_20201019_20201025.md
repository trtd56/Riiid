# 20201019_20201025

## 課題
- BigQueryで特徴量生成→notebookで学習、をやりたかったけど、メモリが厳しそう
- BigQuery MLとNotebookの再現性が取れない
  - とりあえずシンプルなモデルで推論が再現できる確認する
- データ(lecture.csv)が更新されたそう
## 特徴量
- prior_question_elapsed_timeとtimestampの差

## Discussion & Notebook
### [Old lecture.csv for fairness](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191530)
- 古いテストIDが置かれてる
### [Simple LGBM 3 Feature Model](https://www.kaggle.com/dwit392/simple-lgbm-3-feature-model)
- 3つの特徴量だけのシンプルなモデル
- [v19](https://www.kaggle.com/dwit392/simple-lgbm-3-feature-model?scriptVersionId=44299556)でValid 0.7555477308738588 / LB 0.738
- 拡張版: [Expanding on Simple LGBM](https://www.kaggle.com/dwit392/expanding-on-simple-lgbm)
### [RIIID: FTRL FTW !]()
- 広告で使われるオンライン学習モデル
- よさげ
- DIscussion: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191751

### [Does a GrowNet could be useful in this competition?](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/191626)
- 購買BoostingっぽいNN

## メモ
### 実験
#### 3特徴量に絞って
- 0.7035
![](https://www.kaggleusercontent.com/kf/45016693/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..BC9IHvMj7zP-eLHpSly2Lg.mfkmTQX6i9yAcJHAyIbfNEujuPH__ji17qDgOjsHybV3zPJ8AdRVsnV2OIbCrhi8n0By5at4ZuE7JaPzcDlBQMg2V1vrW3SKcOyPw-l-dZhMFljLn0fCEqfEX_nYNxIXT-EaAVLwp40jeazHv3aG7AREZlMmh7C4qTDxw-ygWFlGXkct3PHVGCXgOUhM-S8HlZroFrJsi6_7JcqQektb6EDyJFktJ53N6Yi-omANfdNEJ9_dn8mRTJ9X6WW9kS2tbdXd3RFjpIM8m5-wannGRzGzBGI_9PVr-CYf5_6E-tPZ3ZdN7CVC6WBOgVnm5-CcrLEsX0ev_8zfkVIq6HT8WhAlpSIZLh3GbwnOl_zpQ9zwzyH9ry1cDhQ-EyqcoriEPtmROIV94j0VrdxGBv1WE8cdzIehxMEBy3NiAXWbDibBcLfkMt1zBKjfkAqgcZVMU8C-0zRoiQvHeJaoCBYQzH0dVb9VCXhnZ-3gyyjEvjSY-IijfOzE5WFjqQYl2vqib4qCB-_EPR8wjxhKqlXvHC8eE97DlQUPiYuxrbHpZ4lCQTY95e0Q5IWZB31-VW3yBgn9hgzJ1gz1Td3WHTTc1qxwrMdzTmXED8HtABIWR0wwnIKG28FOSRDatfJfIuQLmaRnB8iDd0Ap91bTo2SjZQ.WinFrbVmgGF5KXhOjRRp-A/__results___files/__results___7_0.png)


### XGBoostのパラメータ([10/17](https://github.com/trtd56/Riiid/blob/master/docs/diary_20201012_20201018/diary_20201017.md)の残り)
- MIN_SPLIT_LOSS = float64_value,
  - ツリーの葉にさらに分割するために必要な最小損失削減
  - 大きいほど、アルゴリズムは保守的≒過学習しにくい
  - デフォルト値は 0
- MAX_TREE_DEPTH = int64_value,
  - ツリーの最大深度
  - デフォルト値は 6
- SUBSAMPLE = float64_value,
  - 学習のサブサンプリング率
  - 例えばこの値を 0.5 に設定すると、ツリーを拡大する前に学習データの半分がランダムにサンプリングされ、過学習を防ぐ
  - サブサンプリングは、ブースト イテレーションごとに 1 回行われる
  - テストデータの分割とは関係ない（デフォルトでは 80 / 20 ランダム）
  - デフォルト値は 1.0 （各イテレーションですべてのトレーニング データが使用される）
- AUTO_CLASS_WEIGHTS = { TRUE | FALSE },
  - 各クラスの頻度に反比例した重みを使用して、クラスラベルのバランスをとるかどうか
  - デフォルト False
- CLASS_WEIGHTS = struct_array,
  - 各クラスラベルに使用する重み
  - STRUCT の ARRAY
  - 例: `CLASS_WEIGHTS = [STRUCT('example_label', .2)]`
- L1_REG = float64_value,
  - 適用する L1 正則化の量
  - デフォルト 0
- L2_REG = float64_value,
  - 適用する L2 正則化の量
  - デフォルト 1.0
- EARLY_STOP = { TRUE | FALSE },
  - early stopを使用するか否か
- LEARN_RATE = float64_value,
  - 学習率
  - デフォルト値 0.3
- INPUT_LABEL_COLS = string_array,
  - 目的変数の列名
- MAX_ITERATIONS = int64_value,
  - ブーストのラウンドの最大数
  - デフォルト20
- MIN_REL_PROGRESS = float64_value,
  - EARLY_STOP が true に設定されている場合に、トレーニングの継続に最低限必要な相対損失改善値
  - たとえば、値を 0.01 に設定した場合、イテレーションごとに損失が 1% 減少すると、トレーニングが継続する。
  - デフォルト値は 0.01

#### その他参考
- [XGBoost公式](https://xgboost.readthedocs.io/en/latest/parameter.html)
- [BigQuery ML XGBoost](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)
- [ハイパラチューニング記事１](https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f)
- [ハイパラチューニング記事2](https://qiita.com/FJyusk56/items/0649f4362587261bd57a)
- [ハイパラチューニング記事3](https://shengyg.github.io/repository/machine%20learning/2017/02/25/Complete-Guide-to-Parameter-Tuning-xgboost.html)

パタメータチューニングに関してはKaggle本呼んだほうが良いかも
